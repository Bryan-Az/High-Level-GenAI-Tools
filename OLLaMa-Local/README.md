# OLLaMA for Running LLMs Locally

In this section, running on the cloud via Google Colab, I'll be using OLLaMA to run really computationally expensive models. OLLaMA provides methods to lower the amount of complexity when hosting large language models on the cloud.